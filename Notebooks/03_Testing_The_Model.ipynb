{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839b0cf6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771030e",
   "metadata": {},
   "source": [
    "`load_model`: Used to load in the models we saved  \n",
    "`datetime`: Used to help name screenshots  \n",
    "`mediapipe`: What will be drawing our hands for easier recognition by models  \n",
    "`numpy`: Needed for just a conversion and finding the what action was chosen  \n",
    "`pyautogui`: What will actually do the actions specficied   \n",
    "`time`: Will be used as a sleep timer as well as naming screenshots  \n",
    "`cv2`: OpenCv is what's used to access the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47f3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a5702",
   "metadata": {},
   "source": [
    "### Make Sure actions are in the same order as read in from `ImageDataGenerator` as order might have changed (alphabetically) from when first made\n",
    "---\n",
    "**Original order was this**\n",
    "<img src = './notebook_imgs/original_indicies.png'>\n",
    "---\n",
    "**`ImageDataGenerator` changed the order to this**\n",
    "<img src = './notebook_imgs/class_indicies.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7dd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['forward', 'play_pause', 'rewind', 'screenshot', 'volume_down', 'volume_up']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8143c5",
   "metadata": {},
   "source": [
    "### Load in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37144b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = load_model('../models/mobilenet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7256b2",
   "metadata": {},
   "source": [
    "### Set up the MediaPipe hands model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d5c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd7ac3",
   "metadata": {},
   "source": [
    "## Running everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd96908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to store the seq and action seq's\n",
    "action_seq = []\n",
    "\n",
    "# instantiate OpenCv\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert image from BGR to RGB to work with mediapipe\n",
    "    image = cv2.flip(image, 1) # flip on horizontal\n",
    "    image.flags.writeable = False    # set flag to False\n",
    "    results = hands.process(image)   # actually makes the detections\n",
    "    image.flags.writeable = True     # set flag back to True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # set color back to BGR\n",
    "    \n",
    "    # draw the mediapipe hands on there\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                     mp_drawing.DrawingSpec(color=(51, 51, 255), thickness = 2, circle_radius=2),\n",
    "                                     mp_drawing.DrawingSpec(color=(0, 0, 0), thickness = 2, circle_radius=2))\n",
    "        \n",
    "        # Our model is expecting an image of size 224 x 224\n",
    "        small_image =  cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA) \n",
    "        \n",
    "        # passing image to the model\n",
    "        y_pred = mn.predict((small_image.reshape(1, 224, 224, 3)/255).astype(np.float32))\n",
    "\n",
    "        # get the image prediction \n",
    "        i_pred = int(np.argmax(y_pred))\n",
    "\n",
    "        conf = y_pred[0][i_pred]\n",
    "\n",
    "        # start loop over again if confidence isn't above 90\n",
    "        if conf < 0.90:\n",
    "            continue\n",
    "\n",
    "        # add the prediction to the action_seq list \n",
    "        action = actions[i_pred]\n",
    "        action_seq.append(action)\n",
    "\n",
    "        # Show '?' if model doesn't know what to do yet if there are multiple poses in action_seq\n",
    "        this_action = '?'\n",
    "\n",
    "        # Make sure there are at least 3 predictions inside the action_seq list \n",
    "        if len(action_seq) > 3:\n",
    "            \n",
    "            # Need to make the same prediction 3 times before making 'this_action' = 'action'\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "\n",
    "                # make it do the actions \n",
    "                if this_action == 'volume_up': # volume up\n",
    "                    pyautogui.press('volumeup')\n",
    "\n",
    "                elif this_action  == 'volume_down': # volume down\n",
    "                    pyautogui.press('volumedown')\n",
    "\n",
    "                elif this_action  == 'screenshot': # screen shot\n",
    "                    now = datetime.now()\n",
    "                    now = time.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "                    pyautogui.screenshot(f'../screenshots/{now}.png')\n",
    "\n",
    "                elif this_action  == 'play_pause': # play\n",
    "                    pyautogui.press('space')\n",
    "                    time.sleep(0.05)\n",
    "                    \n",
    "                elif this_action  == 'forward': # forward\n",
    "                    pyautogui.press('right')\n",
    "\n",
    "                elif this_action  == 'rewind': # rewind\n",
    "                    pyautogui.press('left')\n",
    "\n",
    "            # print out the action that is being done\n",
    "            cv2.putText(image, f'{this_action.upper()}', org= (10, 50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(52, 79, 235), thickness=2)\n",
    "        else:\n",
    "            # if nothing was found, then print out '?'\n",
    "            cv2.putText(image, f'{this_action.upper()}', org= (10, 50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(52, 79, 235), thickness=2)\n",
    "        \n",
    "        # the model takes around 0.9 seconds to make a prediction \n",
    "        time.sleep(.1)\n",
    "        \n",
    "    cv2.imshow('Gesture Control', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
