{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47f3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import pyautogui\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7dd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['play_pause', 'forward', 'rewind', 'volume_up', 'volume_down', 'screen_shot']\n",
    "seq_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538ef9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/cnn_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1da7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to store the seq and action seq's\n",
    "seq = []\n",
    "action_seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d5c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the MediaPipe hands model\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ffc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert image from BGR to RGB to work with mediapipe\n",
    "    image = cv2.flip(image, 1) # flip on horizontal\n",
    "    image.flags.writeable = False    # set flag to False\n",
    "    results = hands.process(image)   # actually makes the detections\n",
    "    image.flags.writeable = True     # set flag back to True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # set color back to BGR\n",
    "\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                     mp_drawing.DrawingSpec(color=(51, 51, 255), thickness = 2, circle_radius=2),\n",
    "                                     mp_drawing.DrawingSpec(color=(0, 0, 0), thickness = 2, circle_radius=2))\n",
    "\n",
    "            \n",
    "            y_pred = model.predict(image.reshape(1, 480, 640, 3))\n",
    "\n",
    "            i_pred = np.argmax(y_pred)\n",
    "#             conf = y_pred[i_pred]\n",
    "\n",
    "#             if conf < 0.9:\n",
    "#                 continue\n",
    "\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "#             if len(action_seq) < 3:\n",
    "#                 continue\n",
    "\n",
    "            this_action = '?'\n",
    "            # basically, hold it for long enough for it to register 3 times before making 'this_action' equal \n",
    "            \n",
    "            try:\n",
    "                if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                    this_action = action\n",
    "\n",
    "                    # make it do the actions \n",
    "\n",
    "                    if this_action == 'volume_up': # volume up\n",
    "                        pyautogui.press('volumeup')\n",
    "\n",
    "                    elif this_action  == 'volume_down': # volume down\n",
    "                        pyautogui.press('volumedown')\n",
    "\n",
    "                    elif this_action  == 'screen_shot': # screen shot\n",
    "                        time = datetime.now()\n",
    "                        time = time.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "                        pyautogui.screenshot(f'./screenshots/{time}.png')\n",
    "\n",
    "                    elif this_action  == 'play': # play\n",
    "                        pyautogui.press('space')\n",
    "\n",
    "                    elif this_action  == 'pause': #pause but will probably change it to something else since play can be both\n",
    "                        pyautogui.press('space')\n",
    "\n",
    "                    elif this_action  == 'forward': # forward\n",
    "                        pyautogui.press('right')\n",
    "\n",
    "                    elif this_action  == 'rewind': # rewind\n",
    "                        pyautogui.press('left')\n",
    "\n",
    "\n",
    "                cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(52, 79, 235), thickness=2)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    cv2.imshow('Gesture Control', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3d6c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf122268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saying that we are only expecting 1 image instead of a big list\n",
    "model.predict(image.reshape(1, 480, 640, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4103a03",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[254, 255, 254],\n",
       "         [254, 255, 254],\n",
       "         [254, 255, 254],\n",
       "         ...,\n",
       "         [213, 210, 197],\n",
       "         [207, 210, 200],\n",
       "         [209, 212, 203]],\n",
       "\n",
       "        [[254, 255, 254],\n",
       "         [254, 255, 254],\n",
       "         [254, 255, 254],\n",
       "         ...,\n",
       "         [210, 208, 195],\n",
       "         [204, 208, 198],\n",
       "         [208, 211, 201]],\n",
       "\n",
       "        [[252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         ...,\n",
       "         [209, 211, 197],\n",
       "         [207, 212, 195],\n",
       "         [206, 211, 194]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         ...,\n",
       "         [ 92, 121, 123],\n",
       "         [ 92, 121, 123],\n",
       "         [ 87, 116, 118]],\n",
       "\n",
       "        [[252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         ...,\n",
       "         [ 86, 117, 125],\n",
       "         [ 86, 117, 125],\n",
       "         [ 87, 118, 127]],\n",
       "\n",
       "        [[252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         [252, 255, 252],\n",
       "         ...,\n",
       "         [ 84, 116, 124],\n",
       "         [ 84, 116, 124],\n",
       "         [ 83, 115, 123]]]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.reshape(1, 480, 640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c13237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1220c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
